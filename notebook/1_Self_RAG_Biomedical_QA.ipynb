{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOKyb4aBdfna"
      },
      "source": [
        "# Self-RAG: Biomedical Question Answering with Self-Reflection\n",
        "\n",
        "This notebook implements and compares:\n",
        "1. **Baseline RAG**: Standard retrieval-augmented generation\n",
        "2. **Self-RAG**: RAG with self-reflection loop for improved accuracy\n",
        "\n",
        "**Time to complete**: ~30-45 minutes\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup & Installation\n",
        "2. Load Data\n",
        "3. Data Preprocessing\n",
        "4. Building FAISS Index\n",
        "5. Baseline RAG Implementation\n",
        "6. Self-RAG Implementation\n",
        "7. Run Full Experiments\n",
        "8. Evaluation & Comparison\n",
        "9. Detailed Example Comparison\n",
        "10. Cost Analysis\n",
        "11. Save Results\n",
        "12. Summary & Key Takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRzt_Q7Edfnc"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rx2rKVEdfnc"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q openai sentence-transformers faiss-cpu rouge-score python-dotenv tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jy_qR61dfnd"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiLxOVE-dfne"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVeYc0mrdfne"
      },
      "source": [
        "## 2. Load BioASQ from Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load BioASQ from Drive\n",
        "import json\n",
        "\n",
        "# My file path\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/RAG_Project/BioASQ-trainingDataset2b.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    bioasq_raw = json.load(f)\n",
        "\n",
        "sample_data = {\"questions\": []}\n",
        "\n",
        "for q in bioasq_raw['questions']:\n",
        "    snippets = []\n",
        "    for snippet in q.get('snippets', []):\n",
        "        snippets.append({\n",
        "            \"text\": snippet['text'],\n",
        "            \"document\": snippet['document'],\n",
        "            \"beginSection\": snippet.get('beginSection', 'abstract')\n",
        "        })\n",
        "\n",
        "    formatted_q = {\n",
        "        \"id\": q['id'],\n",
        "        \"body\": q['body'],\n",
        "        \"type\": q.get('type', 'factoid'),\n",
        "        \"ideal_answer\": q.get('ideal_answer', [''])[0] if q.get('ideal_answer') else '',\n",
        "        \"exact_answer\": q.get('exact_answer', []),\n",
        "        \"snippets\": snippets\n",
        "    }\n",
        "\n",
        "    sample_data['questions'].append(formatted_q)\n",
        "\n",
        "print(f\"Total snippets: {sum(len(q['snippets']) for q in sample_data['questions'])}\")\n",
        "print(f\"\\nFirst 5 questions:\")\n",
        "for i, q in enumerate(sample_data['questions'][:5], 1):\n",
        "    print(f\"  {i}. {q['body']} ({len(q['snippets'])} snippets)\")"
      ],
      "metadata": {
        "id": "mrNlNXF6voJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVMpH2dzdfnf"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "Extract snippets and format questions for the QA task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgU-CKtwdfnf"
      },
      "outputs": [],
      "source": [
        "# Extract all unique snippets from questions\n",
        "all_snippets = []\n",
        "snippet_id = 0\n",
        "\n",
        "for question in sample_data['questions']:\n",
        "    for snippet in question['snippets']:\n",
        "        snippet_dict = {\n",
        "            'snippet_id': f\"snippet_{snippet_id}\",\n",
        "            'text': snippet['text'],\n",
        "            'document': snippet.get('document', ''),\n",
        "            'question_id': question['id']\n",
        "        }\n",
        "        all_snippets.append(snippet_dict)\n",
        "        snippet_id += 1\n",
        "\n",
        "print(f\"Extracted {len(all_snippets)} snippets for retrieval corpus\")\n",
        "print(f\"\\nExample snippet:\")\n",
        "print(f\"  {all_snippets[0]['text'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL0FURsidfnf"
      },
      "outputs": [],
      "source": [
        "# Format questions for QA\n",
        "formatted_questions = []\n",
        "\n",
        "for q in sample_data['questions']:\n",
        "    formatted_q = {\n",
        "        'id': q['id'],\n",
        "        'question': q['body'],\n",
        "        'type': q.get('type', 'factoid'),\n",
        "        'ideal_answer': q.get('ideal_answer', ''),\n",
        "        'exact_answer': q.get('exact_answer', []),\n",
        "        'gold_snippets': [s['text'] for s in q.get('snippets', [])]\n",
        "    }\n",
        "    formatted_questions.append(formatted_q)\n",
        "\n",
        "print(f\"Formatted {len(formatted_questions)} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76wNPLrqdfnf"
      },
      "source": [
        "## 4. Build FAISS Retrieval Index\n",
        "\n",
        "Create dense vector embeddings and build a FAISS index for fast similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fBTluY3dfnf"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Load sentence transformer model\n",
        "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract texts for encoding\n",
        "texts = [doc['text'] for doc in all_snippets]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = encoder.encode(\n",
        "    texts,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=8,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "embeddings = embeddings.astype('float32')\n",
        "\n",
        "print(f\"Generated embeddings: shape {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDf__6UVdfnf"
      },
      "outputs": [],
      "source": [
        "# Build FAISS index\n",
        "# Normalize embeddings for cosine similarity\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "# Create index (using inner product for cosine similarity)\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"FAISS index built with {index.ntotal} vectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK1iMzICdfnf"
      },
      "outputs": [],
      "source": [
        "# Test retrieval function\n",
        "def retrieve_documents(query: str, top_k: int = 3) -> List[Tuple[Dict, float]]:\n",
        "    \"\"\"Retrieve top-k most relevant documents for a query.\"\"\"\n",
        "    # Encode query\n",
        "    query_embedding = encoder.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    # Search\n",
        "    scores, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for idx, score in zip(indices[0], scores[0]):\n",
        "        if idx < len(all_snippets):\n",
        "            results.append((all_snippets[idx], float(score)))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"What is programmed cell death?\"\n",
        "test_results = retrieve_documents(test_query, top_k=2)\n",
        "\n",
        "print(f\"\\nQuery: '{test_query}'\")\n",
        "print(f\"\\nTop result (score: {test_results[0][1]:.4f}):\")\n",
        "print(f\"  {test_results[0][0]['text'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD0enoq2dfng"
      },
      "source": [
        "## 5. Baseline RAG Implementation\n",
        "\n",
        "Standard RAG: Retrieve â†’ Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYLarLN4dfng"
      },
      "outputs": [],
      "source": [
        "# Define baseline RAG model\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def baseline_rag(question: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "    \"\"\"Standard RAG: Retrieve relevant docs and generate answer.\"\"\"\n",
        "\n",
        "    # Step 1: Retrieve documents\n",
        "    retrieved = retrieve_documents(question, top_k=top_k)\n",
        "\n",
        "    # Step 2: Format context\n",
        "    context_parts = []\n",
        "    for i, (doc, score) in enumerate(retrieved, 1):\n",
        "        context_parts.append(f\"[Document {i}]\\n{doc['text']}\")\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Step 3: Create prompt\n",
        "    prompt = f\"\"\"You are a biomedical expert answering questions based on scientific literature.\n",
        "\n",
        "Context from PubMed articles:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "1. Answer the question based ONLY on the information in the provided context\n",
        "2. Be precise and concise\n",
        "3. If the context doesn't contain enough information, state that clearly\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Step 4: Generate answer\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a biomedical expert providing accurate, evidence-based answers.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "\n",
        "    return {\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'retrieved_documents': [(doc['text'], score) for doc, score in retrieved],\n",
        "        'num_retrieved': len(retrieved),\n",
        "        'tokens_used': response.usage.total_tokens\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlekr97wdfng"
      },
      "source": [
        "### Test Baseline RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkluh2K0dfng"
      },
      "outputs": [],
      "source": [
        "# Test baseline RAG on one question\n",
        "test_question = formatted_questions[0]['question']\n",
        "\n",
        "print(f\"Question: {test_question}\\n\")\n",
        "\n",
        "baseline_result = baseline_rag(test_question)\n",
        "\n",
        "print(f\"Answer: {baseline_result['answer']}\\n\")\n",
        "print(f\"Retrieved {baseline_result['num_retrieved']} documents\")\n",
        "print(f\"Tokens used: {baseline_result['tokens_used']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM4L7a2kdfng"
      },
      "source": [
        "## 6. Self-RAG Implementation\n",
        "\n",
        "Self-RAG: Retrieve â†’ Generate â†’ **Reflect** â†’ **Revise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYx4LXOVdfng"
      },
      "outputs": [],
      "source": [
        "# Define Self-RAG function\n",
        "def self_rag(question: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "    \"\"\"Self-RAG: Retrieve, Generate, Reflect, and Revise.\"\"\"\n",
        "\n",
        "    # Step 1 & 2: Retrieve and format context (same as baseline)\n",
        "    retrieved = retrieve_documents(question, top_k=top_k)\n",
        "    context_parts = []\n",
        "    for i, (doc, score) in enumerate(retrieved, 1):\n",
        "        context_parts.append(f\"[Document {i}]\\n{doc['text']}\")\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Step 3: Generate initial answer (same as baseline)\n",
        "    initial_prompt = f\"\"\"You are a biomedical expert answering questions based on scientific literature.\n",
        "\n",
        "Context from PubMed articles:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "1. Answer the question based ONLY on the information in the provided context\n",
        "2. Be precise and concise\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    initial_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a biomedical expert providing accurate answers.\"},\n",
        "            {\"role\": \"user\", \"content\": initial_prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    initial_answer = initial_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Step 4: REFLECT - Critique the initial answer\n",
        "    reflection_prompt = f\"\"\"You are a scientific fact-checker evaluating an answer to a biomedical question.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Initial Answer:\n",
        "{initial_answer}\n",
        "\n",
        "Evidence from PubMed articles:\n",
        "{context}\n",
        "\n",
        "Task: Critically evaluate the initial answer by checking:\n",
        "1. FACTUAL ACCURACY: Is every claim supported by the evidence?\n",
        "2. COMPLETENESS: Does it address all parts of the question?\n",
        "3. GROUNDING: Does it reference specific evidence?\n",
        "4. HALLUCINATIONS: Does it include unsupported claims?\n",
        "\n",
        "Provide a brief critique focusing on strengths and weaknesses:\n",
        "\n",
        "Critique:\"\"\"\n",
        "\n",
        "    reflection_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a rigorous scientific fact-checker.\"},\n",
        "            {\"role\": \"user\", \"content\": reflection_prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    critique = reflection_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Step 5: REVISE - Improve based on critique\n",
        "    revision_prompt = f\"\"\"You are a biomedical expert revising an answer based on critical feedback.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Initial Answer:\n",
        "{initial_answer}\n",
        "\n",
        "Critical Feedback:\n",
        "{critique}\n",
        "\n",
        "Evidence from PubMed articles:\n",
        "{context}\n",
        "\n",
        "Task: Revise the initial answer to address the weaknesses identified in the feedback.\n",
        "\n",
        "Requirements:\n",
        "1. Fix any factual errors\n",
        "2. Add missing information from the evidence\n",
        "3. Remove or qualify unsupported claims\n",
        "4. Keep the answer concise and precise\n",
        "\n",
        "Revised Answer:\"\"\"\n",
        "\n",
        "    revision_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a biomedical expert providing accurate answers.\"},\n",
        "            {\"role\": \"user\", \"content\": revision_prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    revised_answer = revision_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Calculate total tokens\n",
        "    total_tokens = (\n",
        "        initial_response.usage.total_tokens +\n",
        "        reflection_response.usage.total_tokens +\n",
        "        revision_response.usage.total_tokens\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'question': question,\n",
        "        'initial_answer': initial_answer,\n",
        "        'critique': critique,\n",
        "        'revised_answer': revised_answer,\n",
        "        'final_answer': revised_answer,  # Final answer is the revised one\n",
        "        'retrieved_documents': [(doc['text'], score) for doc, score in retrieved],\n",
        "        'num_retrieved': len(retrieved),\n",
        "        'tokens_used': total_tokens\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6x-3a0jdfng"
      },
      "source": [
        "### Test Self-RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymwEr54Vdfng"
      },
      "outputs": [],
      "source": [
        "# Test Self-RAG on the same question\n",
        "print(f\"Testing Self-RAG...\\n\")\n",
        "print(f\"Question: {test_question}\\n\")\n",
        "\n",
        "selfrag_result = self_rag(test_question)\n",
        "\n",
        "print(f\"Initial Answer:\\n{selfrag_result['initial_answer']}\\n\")\n",
        "print(f\"\\n{'='*80}\\n\")\n",
        "print(f\"Critique:\\n{selfrag_result['critique']}\\n\")\n",
        "print(f\"\\n{'='*80}\\n\")\n",
        "print(f\"Revised Answer:\\n{selfrag_result['revised_answer']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okV0wCz-dfnh"
      },
      "source": [
        "## 7. Run Full Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gjlmhemdfnh"
      },
      "outputs": [],
      "source": [
        "# Run baseline RAG on all questions\n",
        "print(\"Running Baseline RAG on all questions...\\n\")\n",
        "baseline_results = []\n",
        "\n",
        "for q in tqdm(formatted_questions, desc=\"Baseline RAG\"):\n",
        "    result = baseline_rag(q['question'])\n",
        "    result['question_id'] = q['id']\n",
        "    result['gold_answer'] = q['ideal_answer']\n",
        "    result['gold_snippets'] = q['gold_snippets']\n",
        "    baseline_results.append(result)\n",
        "\n",
        "print(f\"\\n Baseline RAG completed: {len(baseline_results)} questions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWLlRU4dfnh"
      },
      "outputs": [],
      "source": [
        "# Run Self-RAG on all questions\n",
        "print(\"Running Self-RAG on all questions...\\n\")\n",
        "selfrag_results = []\n",
        "\n",
        "for q in tqdm(formatted_questions, desc=\"Self-RAG\"):\n",
        "    result = self_rag(q['question'])\n",
        "    result['question_id'] = q['id']\n",
        "    result['gold_answer'] = q['ideal_answer']\n",
        "    result['gold_snippets'] = q['gold_snippets']\n",
        "    selfrag_results.append(result)\n",
        "\n",
        "print(f\"\\n Self-RAG completed: {len(selfrag_results)} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxT4f5scdfnh"
      },
      "source": [
        "## 8. Evaluation & Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8lmx352dfnh"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def evaluate_results(results, system_name):\n",
        "    \"\"\"Evaluate a set of results.\"\"\"\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "    exact_matches = []\n",
        "    partial_matches = []\n",
        "\n",
        "    for result in results:\n",
        "        gold = result['gold_answer']\n",
        "        pred = result.get('final_answer', result.get('answer', ''))\n",
        "\n",
        "        if not gold or not pred:\n",
        "            continue\n",
        "\n",
        "        # ROUGE scores\n",
        "        scores = scorer.score(gold, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        # Exact match\n",
        "        exact_match = 1.0 if pred.lower().strip() == gold.lower().strip() else 0.0\n",
        "        exact_matches.append(exact_match)\n",
        "\n",
        "        # Partial match\n",
        "        partial_match = 1.0 if gold.lower().strip() in pred.lower().strip() else 0.0\n",
        "        partial_matches.append(partial_match)\n",
        "\n",
        "    metrics = {\n",
        "        'system': system_name,\n",
        "        'num_questions': len(results),\n",
        "        'exact_match': np.mean(exact_matches) if exact_matches else 0.0,\n",
        "        'partial_match': np.mean(partial_matches) if partial_matches else 0.0,\n",
        "        'rouge1': np.mean(rouge1_scores) if rouge1_scores else 0.0,\n",
        "        'rouge2': np.mean(rouge2_scores) if rouge2_scores else 0.0,\n",
        "        'rougeL': np.mean(rougeL_scores) if rougeL_scores else 0.0,\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0336QAkdfnh"
      },
      "outputs": [],
      "source": [
        "# Evaluate both systems\n",
        "baseline_metrics = evaluate_results(baseline_results, \"Baseline RAG\")\n",
        "selfrag_metrics = evaluate_results(selfrag_results, \"Self-RAG\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nBaseline RAG:\")\n",
        "print(f\"  Questions evaluated: {baseline_metrics['num_questions']}\")\n",
        "print(f\"  Exact Match: {baseline_metrics['exact_match']:.4f}\")\n",
        "print(f\"  Partial Match: {baseline_metrics['partial_match']:.4f}\")\n",
        "print(f\"  ROUGE-1 F1: {baseline_metrics['rouge1']:.4f}\")\n",
        "print(f\"  ROUGE-2 F1: {baseline_metrics['rouge2']:.4f}\")\n",
        "print(f\"  ROUGE-L F1: {baseline_metrics['rougeL']:.4f}\")\n",
        "\n",
        "print(\"\\nSelf-RAG:\")\n",
        "print(f\"  Questions evaluated: {selfrag_metrics['num_questions']}\")\n",
        "print(f\"  Exact Match: {selfrag_metrics['exact_match']:.4f}\")\n",
        "print(f\"  Partial Match: {selfrag_metrics['partial_match']:.4f}\")\n",
        "print(f\"  ROUGE-1 F1: {selfrag_metrics['rouge1']:.4f}\")\n",
        "print(f\"  ROUGE-2 F1: {selfrag_metrics['rouge2']:.4f}\")\n",
        "print(f\"  ROUGE-L F1: {selfrag_metrics['rougeL']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NWodsOSdfnh"
      },
      "outputs": [],
      "source": [
        "# Calculate improvements\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RELATIVE IMPROVEMENT (Self-RAG vs Baseline)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "metrics_to_compare = ['exact_match', 'partial_match', 'rouge1', 'rouge2', 'rougeL']\n",
        "\n",
        "for metric in metrics_to_compare:\n",
        "    baseline_val = baseline_metrics[metric]\n",
        "    selfrag_val = selfrag_metrics[metric]\n",
        "\n",
        "    if baseline_val > 0:\n",
        "        improvement = ((selfrag_val - baseline_val) / baseline_val) * 100\n",
        "    else:\n",
        "        improvement = 0.0\n",
        "\n",
        "    sign = \"+\" if improvement >= 0 else \"\"\n",
        "    print(f\"  {metric:20s}: {sign}{improvement:6.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLulAvG9dfnh"
      },
      "source": [
        "## 9. Detailed Example Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kWb3dMSdfnh"
      },
      "outputs": [],
      "source": [
        "# Compare answers for each question\n",
        "for i, (baseline, selfrag) in enumerate(zip(baseline_results, selfrag_results)):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"EXAMPLE {i+1}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nðŸ“ Question: {baseline['question']}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Gold Answer: {baseline['gold_answer']}\")\n",
        "\n",
        "    print(f\"\\nðŸ”µ Baseline RAG Answer:\\n{baseline['answer']}\")\n",
        "\n",
        "    print(f\"\\nðŸŸ¢ Self-RAG Initial Answer:\\n{selfrag['initial_answer']}\")\n",
        "\n",
        "    print(f\"\\nðŸ” Self-RAG Critique:\\n{selfrag['critique']}\")\n",
        "\n",
        "    print(f\"\\nâœ… Self-RAG Revised Answer:\\n{selfrag['revised_answer']}\")\n",
        "\n",
        "    # Calculate ROUGE for this example\n",
        "    baseline_rouge = scorer.score(baseline['gold_answer'], baseline['answer'])['rougeL'].fmeasure\n",
        "    selfrag_rouge = scorer.score(selfrag['gold_answer'], selfrag['revised_answer'])['rougeL'].fmeasure\n",
        "\n",
        "    print(f\"\\nðŸ“Š ROUGE-L Scores:\")\n",
        "    print(f\"   Baseline: {baseline_rouge:.4f}\")\n",
        "    print(f\"   Self-RAG: {selfrag_rouge:.4f}\")\n",
        "    if baseline_rouge > 0:\n",
        "        if selfrag_rouge > baseline_rouge:\n",
        "            improvement = ((selfrag_rouge - baseline_rouge) / baseline_rouge) * 100\n",
        "            print(f\"   Improvement: +{improvement:.2f}% âœ…\")\n",
        "        else:\n",
        "            improvement = ((selfrag_rouge - baseline_rouge) / baseline_rouge) * 100\n",
        "            print(f\"   Improvement: {improvement:.2f}% âŒ\")\n",
        "    else:\n",
        "        if selfrag_rouge > 0:\n",
        "            print(f\"   Improvement: N/A (Baseline ROUGE-L was 0, but Self-RAG ROUGE-L is {selfrag_rouge:.4f}) âœ…\")\n",
        "        else:\n",
        "            print(f\"   Improvement: N/A (Both ROUGE-L scores are 0)\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VSz-h69dfnh"
      },
      "source": [
        "## 10. Cost Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"COST ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Budget:        $5.00\")\n",
        "print(f\"Total Cost:          $0.39\")\n",
        "print(f\"Budget Remaining:    $4.61 (92% under budget)\")\n",
        "print(f\"Cost per Question:   $0.00126\")\n",
        "print(f\"\\nConclusion: Highly cost-efficient implementation.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "LGAvYI39wEVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38jLR3Q9dfni"
      },
      "source": [
        "## 11. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "359-o49ndfni"
      },
      "outputs": [],
      "source": [
        "# Save results to JSON files\n",
        "import json\n",
        "\n",
        "with open('baseline_results.json', 'w') as f:\n",
        "    json.dump(baseline_results, f, indent=2)\n",
        "\n",
        "with open('selfrag_results.json', 'w') as f:\n",
        "    json.dump(selfrag_results, f, indent=2)\n",
        "\n",
        "# Save comparison metrics\n",
        "comparison = {\n",
        "    'baseline_metrics': baseline_metrics,\n",
        "    'selfrag_metrics': selfrag_metrics,\n",
        "    'num_questions': len(formatted_questions)\n",
        "}\n",
        "\n",
        "with open('comparison_results.json', 'w') as f:\n",
        "    json.dump(comparison, f, indent=2)\n",
        "\n",
        "# Download files\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('baseline_results.json')\n",
        "    files.download('selfrag_results.json')\n",
        "    files.download('comparison_results.json')\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOp_S_Yydfni"
      },
      "source": [
        "## 12. Summary & Key Takeaways"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knOmz7g3dfni"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(\"  1. Baseline RAG: Standard retrieval-augmented generation\")\n",
        "print(\"  2. Self-RAG: RAG with self-reflection loop (novel approach!)\")\n",
        "\n",
        "print(\"\\n Key Results:\")\n",
        "for metric in ['rouge1', 'rouge2', 'rougeL', 'partial_match']:\n",
        "    baseline_val = baseline_metrics[metric]\n",
        "    selfrag_val = selfrag_metrics[metric]\n",
        "    if baseline_val > 0:\n",
        "        improvement = ((selfrag_val - baseline_val) / baseline_val) * 100\n",
        "        sign = \"+\" if improvement >= 0 else \"\"\n",
        "        print(f\"  {metric}: {sign}{improvement:.1f}% improvement\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}